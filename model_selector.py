#!/usr/bin/env python3
"""
æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨ v1.0
æ ¹æ®ä»»åŠ¡ç±»å‹æ™ºèƒ½é€‰æ‹©æœ€ä¼˜æ¨¡å‹ï¼Œå¹¶è‡ªåŠ¨åˆ‡æ¢

ä¼˜å…ˆçº§ç­–ç•¥ï¼š
1. Gemini Pro ä»˜è´¹ (æœ€é«˜ä¼˜å…ˆçº§)
2. å…è´¹ API (Google Gemini Flash, SiliconFlow ç­‰)
3. å…¶ä»–ä»˜è´¹ API (Claude, GPT-4 ç­‰)

Author: OpenCode AI Assistant
"""

import json
import re
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# é¢œè‰²è¾“å‡º
class Colors:
    RESET = '\033[0m'
    BOLD = '\033[1m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    MAGENTA = '\033[35m'
    CYAN = '\033[36m'
    
    @staticmethod
    def cyan(s): return f"{Colors.CYAN}{s}{Colors.RESET}"
    @staticmethod
    def green(s): return f"{Colors.GREEN}{s}{Colors.RESET}"
    @staticmethod
    def yellow(s): return f"{Colors.YELLOW}{s}{Colors.RESET}"
    @staticmethod
    def magenta(s): return f"{Colors.MAGENTA}{s}{Colors.RESET}"
    @staticmethod
    def bold(s): return f"{Colors.BOLD}{s}{Colors.RESET}"

# ç›´æ¥ import dispatcher
sys.path.insert(0, str(Path(__file__).parent))
from smart_model_dispatcher import SmartModelDispatcher


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    CODING = "coding"           # ç¼–ç¨‹å¼€å‘
    ANALYSIS = "analysis"        # ä»£ç åˆ†æ
    DEBUGGING = "debugging"     # è°ƒè¯•æ’é”™
    WRITING = "writing"         # æ–‡æ¡£å†™ä½œ
    TRANSLATION = "translation" # ç¿»è¯‘
    CHAT = "chat"              # æ—¥å¸¸å¯¹è¯
    RESEARCH = "research"       # ç ”ç©¶æŸ¥è¯¢
    CREATIVE = "creative"      # åˆ›æ„å†…å®¹
    MATH = "math"              # æ•°å­¦è®¡ç®—
    GENERAL = "general"         # é€šç”¨ä»»åŠ¡


class Priority(Enum):
    """æ¨¡å‹ä¼˜å…ˆçº§"""
    TIER_1 = 1  # Gemini Pro ä»˜è´¹ (æœ€é«˜)
    TIER_2 = 2  # å…è´¹æ¨¡å‹
    TIER_3 = 3  # å…¶ä»–ä»˜è´¹


@dataclass
class Model:
    """æ¨¡å‹é…ç½®"""
    id: str
    name: str
    provider: str
    priority: Priority
    strengths: List[str]      # æ“…é•¿é¢†åŸŸ
    weaknesses: List[str]     # ä¸æ“…é•¿é¢†åŸŸ
    cost_per_1k_tokens: float
    context_window: int
    speed: str               # fast/medium/slow
    available: bool = True


class TaskAnalyzer:
    """ä»»åŠ¡ç±»å‹åˆ†æå™¨ - æ”¯æŒä¸­è‹±æ–‡å…³é”®è¯"""
    
    # ä»»åŠ¡ç±»å‹å…³é”®è¯ (ä¼˜åŒ–ä¸­æ–‡æ”¯æŒ)
    _PATTERN_STRINGS = {
        TaskType.CODING: [
            r'(code|ç¼–ç¨‹|å‡½æ•°|class|def|import|api|ç®—æ³•|implement|refactor|å†™ä»£ç |å†™ä¸€ä¸ª|å†™ä¸ª|å†™ä¸ªå‡½æ•°|å†™æ®µä»£ç |å†™ç¨‹åº|ä»£ç |ç¨‹åº|å¼€å‘)',
            r'\b(write|create|build|develop|make)\s+(code|program|function|app|tool)\b',
            r'\.(py|js|ts|java|cpp|c|go|rs|jsx|tsx|vue|swift)$',
            r'(æ’åº|ç®—æ³•|å‡½æ•°|ç±»|æ¥å£|æ¨¡å—|å‰ç«¯|åç«¯|å…¨æ ˆ|web|app|è„šæœ¬)',
        ],
        TaskType.ANALYSIS: [
            r'\b(analyze|analysis|review)\b',
            r'\b(code|project|system|architecture)\s+(review|analysis|audit)\b',
            r'(åˆ†æ|æ£€æŸ¥|å®¡æŸ¥|è§£é‡Š|ç†è§£|ä»£ç å®¡æŸ¥|æ€§èƒ½åˆ†æ|ä¼˜åŒ–å»ºè®®)',
        ],
        TaskType.DEBUGGING: [
            r'\b(debug|error|bug|fix|crash|exception|fail)\b',
            r"\b(Not working|doesn't work|broken|wrong)\b",
            r'Traceback|Exception|Error|Stack trace',
            r'(é”™è¯¯|ä¿®å¤|å´©æºƒ|é—®é¢˜|bug|è°ƒè¯•|æŠ¥é”™|é—ªé€€|å¡æ­»)',
        ],
        TaskType.WRITING: [
            r'\b(write|create|draft|compose)\s+(doc|article|post|blog|readme|email)\b',
            r'\b(summarize|rewrite|edit|improve|polish)\b',
            r'\.(md|txt|doc|rst)$',
            r'(å†™æ–‡ç« |å†™æ–‡æ¡£|å†™README|å†™åšå®¢|å†™æŠ¥å‘Š|æ€»ç»“|æ”¹å†™)',
        ],
        TaskType.TRANSLATION: [
            r'\b(translate|translation|convert)\b',
            r'\bä¸­æ–‡|è‹±æ–‡|Japanese|Korean|French|German|Spanish\b',
            r'(ç¿»è¯‘|ä»€ä¹ˆæ„æ€|æ€ä¹ˆå†™|å¦‚ä½•è¯´)',
        ],
        TaskType.CHAT: [
            r'^hi|^hello|^hey',
            r"\b(what's up|how are you)\b",
            r'^(!|\?|)[a-zA-Z\s]{0,20}$',
            r'(ä½ å¥½|åœ¨å—|å—¨|hey|hi|hello|èŠèŠ)',
        ],
        TaskType.RESEARCH: [
            r'\b(research|find|search|lookup|look up)\b',
            r'\b(latest|newest|recent|2024|2025|2026)\b',
            r'\b(compare|versus|vs|pros and cons)\b',
            r'(æœç´¢|æŸ¥æ‰¾|ç ”ç©¶|å¯¹æ¯”|æ¯”è¾ƒ|åŒºåˆ«|å“ªä¸ªå¥½)',
        ],
        TaskType.CREATIVE: [
            r'\b(create|generate|design|imagine)\s+(story|poem|song|idea|concept)\b',
            r'\b(creative|original|innovative)\b',
            r'\b(brainstorm)\b',
            r'(åˆ›æ„|å¤´è„‘é£æš´|å†™æ•…äº‹|å†™è¯—|åˆ›ä½œ|è®¾è®¡|æƒ³è±¡)',
        ],
        TaskType.MATH: [
            r'\b(calculate|compute|solve)\s+(equation|integral|derivative|math)\b',
            r'[+\-*/^%=|<>]',
            r'\d+[\d,\.]*\d*[\+\-\*/]\d+[\d,\.]*\d*',
            r'(è®¡ç®—|æ•°å­¦|æ–¹ç¨‹|ç§¯åˆ†|å¾®åˆ†|ç®—ä¸€ä¸‹)',
        ],
    }
    
    # é¢„ç¼–è¯‘æ‰€æœ‰æ­£åˆ™è¡¨è¾¾å¼
    PATTERNS = {k: [re.compile(p, re.IGNORECASE) for p in v] for k, v in _PATTERN_STRINGS.items()}
    
    # å¤æ‚åº¦æŒ‡æ ‡ (é¢„ç¼–è¯‘)
    _COMPLEXITY_STRINGS = [
        r'\b(complex|difficult|hard|advanced|expert|professional)\b',
        r'\b(architecture|system design|microservice|distributed)\b',
        r'\b(optimization|performance|scalability)\b',
        r'\b(500|1000|10000)\+',
        r'\b(million|billion|trillion)\b',
        r'\b(multi-|cross-|poly-)\b',
        r'(å¤æ‚|å›°éš¾|é«˜çº§|ä¸“å®¶|æ¶æ„|ç³»ç»Ÿè®¾è®¡|å¾®æœåŠ¡|åˆ†å¸ƒå¼|ä¼˜åŒ–|æ€§èƒ½|é«˜å¹¶å‘)',
    ]
    COMPLEXITY_PATTERNS = [re.compile(p, re.IGNORECASE) for p in _COMPLEXITY_STRINGS]
    
    # ç´§æ€¥ä»»åŠ¡æŒ‡æ ‡ (é¢„ç¼–è¯‘)
    _URGENT_STRINGS = [
        r'\b(urgent|ASAP|immediately|now|quick|fast)\b',
        r'\b(before|deadline|due)\b',
        r'\b(broken|critical|emergency|help)\b',
        r'(ç´§æ€¥|é©¬ä¸Š|ç«‹åˆ»|ç«‹å³|ç€æ€¥|deadline|æˆªæ­¢|å´©æºƒ|ä¸¥é‡|æ•‘å‘½)',
    ]
    URGENT_PATTERNS = [re.compile(p, re.IGNORECASE) for p in _URGENT_STRINGS]
    
    def __init__(self, task: str):
        self.task = task.lower()
    
    def classify(self) -> TaskType:
        """åˆ†æä»»åŠ¡ç±»å‹"""
        scores = {}
        
        for task_type, compiled_patterns in self.PATTERNS.items():
            score = 0
            for pattern in compiled_patterns:
                if pattern.search(self.task):
                    score += 1
            scores[task_type] = score
        
        best_type = max(scores, key=lambda k: scores[k])
        
        if scores[best_type] == 0:
            return TaskType.GENERAL
        return best_type
    
    def get_complexity(self) -> float:
        """è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦ (0.0 - 1.0)"""
        score = 0
        for pattern in self.COMPLEXITY_PATTERNS:
            if pattern.search(self.task):
                score += 0.15
        
        return min(1.0, score)
    
    def is_urgent(self) -> bool:
        """æ˜¯å¦ç´§æ€¥ä»»åŠ¡"""
        for pattern in self.URGENT_PATTERNS:
            if pattern.search(self.task):
                return True
        return False


class APIHealthChecker:
    """API å¥åº·æ£€æŸ¥å™¨ - å¿«é€Ÿæ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§"""
    
    # Provider ç«¯ç‚¹é…ç½®
    PROVIDER_ENDPOINTS = {
        "google": ("https://generativelanguage.googleapis.com/v1beta/models?key={key}", "google_api_key"),
        "anthropic": ("https://api.anthropic.com/v1/models", "anthropic_api_key"),
        "deepseek": ("https://api.deepseek.com/v1/models", "deepseek_api_key"),
        "siliconflow": ("https://api.siliconflow.cn/v1/models", "siliconflow_api_key"),
        "openai": ("https://api.openai.com/v1/models", "openai_api_key"),
    }
    
    def __init__(self, cache_ttl: int = 60):
        """åˆå§‹åŒ–
        
        Args:
            cache_ttl: ç¼“å­˜æœ‰æ•ˆæœŸ(ç§’)ï¼Œé»˜è®¤60ç§’
        """
        import time
        self._cache: Dict[str, tuple] = {}
        self._cache_ttl = cache_ttl
        self._cache_time: Dict[str, float] = {}
    
    def _load_api_keys(self) -> Dict[str, str]:
        """ä» auth.json åŠ è½½ API keys"""
        import json
        from pathlib import Path
        
        auth_config = Path.home() / ".local" / "share" / "opencode" / "auth.json"
        if not auth_config.exists():
            return {}
        
        try:
            with open(auth_config, 'r') as f:
                data = json.load(f)
        except Exception:
            return {}
        
        # è½¬æ¢ provider_map
        key_map = {}
        for provider, (_, key_name) in self.PROVIDER_ENDPOINTS.items():
            if key_name in data:
                key_map[provider] = data[key_name]
        
        return key_map
    
    def check_provider(self, provider: str) -> bool:
        """æ£€æŸ¥å•ä¸ª provider æ˜¯å¦å¯ç”¨"""
        import time
        import requests
        
        # æ£€æŸ¥ç¼“å­˜
        if provider in self._cache:
            if time.time() - self._cache_time.get(provider, 0) < self._cache_ttl:
                return self._cache[provider]
        
        # åŠ è½½ keys
        keys = self._load_api_keys()
        api_key = keys.get(provider)
        
        if not api_key:
            self._cache[provider] = False
            self._cache_time[provider] = time.time()
            return False
        
        # å¿«é€Ÿå¥åº·æ£€æŸ¥ (2ç§’è¶…æ—¶)
        endpoint, _ = self.PROVIDER_ENDPOINTS.get(provider, ("", ""))
        if not endpoint:
            return False
        
        try:
            if "{key}" in endpoint:
                url = endpoint.format(key=api_key)
                response = requests.get(url, timeout=2)
            else:
                headers = {"Authorization": f"Bearer {api_key}"}
                response = requests.get(url, headers=headers, timeout=2)
            
            is_healthy = response.status_code == 200
            self._cache[provider] = is_healthy
            self._cache_time[provider] = time.time()
            return is_healthy
        except Exception:
            self._cache[provider] = False
            self._cache_time[provider] = time.time()
            return False
    
    def get_available_providers(self) -> Dict[str, bool]:
        """è·å–æ‰€æœ‰ provider çš„å¯ç”¨çŠ¶æ€"""
        return {provider: self.check_provider(provider) 
                for provider in self.PROVIDER_ENDPOINTS.keys()}


class SmartModelSelector:
    """æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨ - æ”¯æŒåŠ¨æ€ API å¯ç”¨æ€§"""
    
    # æ¨¡å‹é…ç½®
    MODELS = {
        # Tier 1: Gemini Pro ä»˜è´¹ (æœ€é«˜ä¼˜å…ˆçº§)
        "gemini-1.5-pro": Model(
            id="gemini-1.5-pro",
            name="Gemini 1.5 Pro",
            provider="google",
            priority=Priority.TIER_1,
            strengths=["coding", "analysis", "reasoning", "long-context", "multimodal"],
            weaknesses=["creative-writing"],
            cost_per_1k_tokens=0.0025,
            context_window=2000000,
            speed="medium",
        ),
        "gemini-2.0-pro": Model(
            id="gemini-2.0-pro",
            name="Gemini 2.0 Pro",
            provider="google",
            priority=Priority.TIER_1,
            strengths=["coding", "analysis", "reasoning", "long-context", "math", "science"],
            weaknesses=["creative"],
            cost_per_1k_tokens=0.003,
            context_window=2000000,
            speed="fast",
        ),
        
        # Tier 2: å…è´¹æ¨¡å‹
        "gemini-1.5-flash": Model(
            id="gemini-1.5-flash",
            name="Gemini 1.5 Flash",
            provider="google",
            priority=Priority.TIER_2,
            strengths=["fast", "chat", "translation", "simple-coding"],
            weaknesses=["deep-analysis", "complex-reasoning"],
            cost_per_1k_tokens=0.000075,
            context_window=1000000,
            speed="fast",
        ),
        "gemini-1.5-flash-8b": Model(
            id="gemini-1.5-flash-8b",
            name="Gemini 1.5 Flash-8B",
            provider="google",
            priority=Priority.TIER_2,
            strengths=["fast", "simple-tasks", "chat"],
            weaknesses=["complex-tasks"],
            cost_per_1k_tokens=0.000075,
            context_window=1000000,
            speed="fastest",
        ),
        "qwen-2.5-72b": Model(
            id="qwen-2.5-72b",
            name="Qwen 2.5 72B",
            provider="siliconflow",
            priority=Priority.TIER_2,
            strengths=["coding", "math", "chinese", "reasoning"],
            weaknesses=["english-creative"],
            cost_per_1k_tokens=0.00014,
            context_window=131072,
            speed="fast",
        ),
        "deepseek-chat": Model(
            id="deepseek-chat",
            name="DeepSeek Chat",
            provider="deepseek",
            priority=Priority.TIER_2,
            strengths=["coding", "reasoning", "cost-effective"],
            weaknesses=["creative-writing"],
            cost_per_1k_tokens=0.00014,
            context_window=128000,
            speed="fast",
        ),
        
        # Tier 3: å…¶ä»–ä»˜è´¹æ¨¡å‹
        "claude-3.5-sonnet": Model(
            id="claude-3.5-sonnet",
            name="Claude 3.5 Sonnet",
            provider="anthropic",
            priority=Priority.TIER_3,
            strengths=["coding", "analysis", "writing", "reasoning", "safety"],
            weaknesses=["speed"],
            cost_per_1k_tokens=0.015,
            context_window=200000,
            speed="medium",
        ),
        "claude-3.7-sonnet": Model(
            id="claude-3.7-sonnet",
            name="Claude 3.7 Sonnet",
            provider="anthropic",
            priority=Priority.TIER_3,
            strengths=["coding", "analysis", "complex-reasoning", "writing"],
            weaknesses=["speed"],
            cost_per_1k_tokens=0.015,
            context_window=200000,
            speed="medium",
        ),
        "gpt-4o": Model(
            id="gpt-4o",
            name="GPT-4o",
            provider="openai",
            priority=Priority.TIER_3,
            strengths=["multimodal", "coding", "analysis", "chat"],
            weaknesses=["cost"],
            cost_per_1k_tokens=0.01,
            context_window=128000,
            speed="medium",
        ),
        "gpt-4o-mini": Model(
            id="gpt-4o-mini",
            name="GPT-4o Mini",
            provider="openai",
            priority=Priority.TIER_3,
            strengths=["fast", "cost-effective", "simple-tasks"],
            weaknesses=["complex-reasoning"],
            cost_per_1k_tokens=0.0006,
            context_window=128000,
            speed="fast",
        ),
    }
    
    # ä»»åŠ¡ç±»å‹ -> æœ€ä½³æ¨¡å‹åŒ¹é…
    TASK_MODEL_MAP = {
        TaskType.CODING: [
            "gemini-2.0-pro",
            "claude-3.7-sonnet",
            "qwen-2.5-72b",
            "deepseek-chat",
            "gemini-1.5-pro",
        ],
        TaskType.ANALYSIS: [
            "gemini-2.0-pro",
            "claude-3.5-sonnet",
            "gpt-4o",
            "gemini-1.5-flash",
        ],
        TaskType.DEBUGGING: [
            "claude-3.7-sonnet",
            "gemini-2.0-pro",
            "deepseek-chat",
            "gpt-4o-mini",
        ],
        TaskType.WRITING: [
            "claude-3.5-sonnet",
            "gpt-4o",
            "gemini-1.5-pro",
        ],
        TaskType.TRANSLATION: [
            "gemini-1.5-flash",
            "qwen-2.5-72b",
            "deepseek-chat",
        ],
        TaskType.CHAT: [
            "gemini-1.5-flash",
            "gemini-1.5-flash-8b",
            "gpt-4o-mini",
        ],
        TaskType.RESEARCH: [
            "gemini-2.0-pro",
            "claude-3.5-sonnet",
            "gpt-4o",
        ],
        TaskType.CREATIVE: [
            "claude-3.5-sonnet",
            "gemini-1.5-pro",
            "gpt-4o",
        ],
        TaskType.MATH: [
            "gemini-2.0-pro",
            "qwen-2.5-72b",
            "deepseek-chat",
        ],
        TaskType.GENERAL: [
            "gemini-1.5-pro",
            "claude-3.5-sonnet",
            "gemini-1.5-flash",
        ],
    }
    
    def __init__(self, available_keys: Optional[Dict[str, bool]] = None, enable_health_check: bool = True):
        """åˆå§‹åŒ–æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨
        
        Args:
            available_keys: é™æ€å¯ç”¨æ€§é…ç½® (å¯é€‰)
            enable_health_check: æ˜¯å¦å¯ç”¨åŠ¨æ€å¥åº·æ£€æŸ¥ (é»˜è®¤å¯ç”¨)
        """
        self._health_checker = APIHealthChecker() if enable_health_check else None
        self._static_available_keys = available_keys or {
            "google": True,
            "anthropic": True,
            "deepseek": True,
            "siliconflow": True,
            "openai": True,
        }
        
        # åˆå§‹åŒ–æ—¶è·å–åŠ¨æ€å¥åº·çŠ¶æ€
        self._dynamic_health: Dict[str, bool] = {}
        if self._health_checker:
            try:
                self._dynamic_health = self._health_checker.get_available_providers()
            except Exception:
                pass
        
        # åˆå¹¶é™æ€å’ŒåŠ¨æ€å¯ç”¨æ€§ (åŠ¨æ€ä¼˜å…ˆ)
        for model_id, model in self.MODELS.items():
            provider = model.provider
            # åŠ¨æ€æ£€æŸ¥ç»“æœä¼˜å…ˆï¼Œå¦åˆ™ä½¿ç”¨é™æ€é…ç½®
            if provider in self._dynamic_health:
                model.available = self._dynamic_health[provider]
            else:
                model.available = self._static_available_keys.get(provider, True)
    
    def select(self, task: str) -> Tuple[Model, str]:
        analyzer = TaskAnalyzer(task)
        task_type = analyzer.classify()
        complexity = analyzer.get_complexity()
        is_urgent = analyzer.is_urgent()
        
        # [æˆæœ¬ä¼˜åŒ–] é•¿æ–‡æœ¬é™çº§ç­–ç•¥ - è¶…è¿‡ 8000 tokens è‡ªåŠ¨åˆ‡æ¢å…è´¹æ¨¡å‹
        estimated_tokens = len(task) // 4  # ç²—ç•¥ä¼°ç®—: 4 å­—ç¬¦ â‰ˆ 1 token
        LONG_TEXT_THRESHOLD = 8000
        
        # åªæœ‰é coding ä»»åŠ¡æ‰è§¦å‘é•¿æ–‡æœ¬é™çº§ (coding éœ€è¦é«˜å¤æ‚åº¦æ¨¡å‹)
        if estimated_tokens > LONG_TEXT_THRESHOLD and task_type != TaskType.CODING:
            logger.info(f"ğŸ“ æ£€æµ‹åˆ°é•¿æ–‡æœ¬ ({estimated_tokens} tokens)ï¼Œå¯ç”¨æˆæœ¬ä¼˜åŒ–ç­–ç•¥")
            # ä¼˜å…ˆé€‰æ‹©å…è´¹é•¿ä¸Šä¸‹æ–‡æ¨¡å‹
            free_long_context = ["gemini-1.5-flash", "qwen-2.5-72b"]
            for model_id in free_long_context:
                if model_id in self.MODELS and self.MODELS[model_id].available:
                    return self.MODELS[model_id], f"ğŸ“ é•¿æ–‡æœ¬ä¼˜åŒ–: {estimated_tokens} tokens > {LONG_TEXT_THRESHOLD}ï¼Œè‡ªåŠ¨é™çº§åˆ°å…è´¹æ¨¡å‹"
        
        candidates = self.TASK_MODEL_MAP.get(task_type, self.TASK_MODEL_MAP[TaskType.GENERAL])
        
        cost_sensitive_tasks = {TaskType.CHAT, TaskType.TRANSLATION, TaskType.GENERAL}
        is_cost_sensitive = task_type in cost_sensitive_tasks and not is_urgent and complexity < 0.5
        
        if complexity > 0.7:
            candidates = [c for c in candidates if self.MODELS.get(c, self.MODELS["gemini-1.5-pro"]).priority.value <= 2]
        elif is_urgent:
            speed_rank = {"fastest": 0, "fast": 1, "medium": 2}
            candidates = sorted(candidates, 
                            key=lambda c: speed_rank.get(self.MODELS.get(c, self.MODELS["gemini-1.5-flash"]).speed, 1))
        
        if is_cost_sensitive:
            candidates = sorted(
                candidates,
                key=lambda c: self.MODELS.get(c, self.MODELS["gemini-1.5-flash"]).cost_per_1k_tokens
            )
        
        for model_id in candidates:
            model = self.MODELS.get(model_id)
            if model and model.available:
                reason = self._generate_reason(task_type, complexity, model, is_cost_sensitive)
                return model, reason
        
        for model_id in candidates:
            model = self.MODELS.get(model_id)
            if model:
                reason = f"å¤‡é€‰æ–¹æ¡ˆ: {model.name}"
                return model, reason
        
        fallback = self.MODELS["gemini-1.5-flash"]
        return fallback, "å…œåº•é€‰æ‹©: å…è´¹å¿«é€Ÿæ¨¡å‹"
    
    def _generate_reason(self, task_type: TaskType, complexity: float, model: Model, is_cost_sensitive: bool = False) -> str:
        reasons = []
        
        if is_cost_sensitive:
            reasons.append(f"ğŸ’° æˆæœ¬ä¼˜åŒ–ï¼š${model.cost_per_1k_tokens:.4f}/1K tokens")
        elif model.priority == Priority.TIER_1:
            reasons.append("ğŸ¯ é¦–é€‰ï¼šGemini Pro ä»˜è´¹ç‰ˆ")
        elif model.priority == Priority.TIER_2:
            reasons.append("ğŸ’° ä¼˜åŒ–ï¼šå…è´¹æ¨¡å‹ï¼Œæ€§ä»·æ¯”é«˜")
        else:
            reasons.append("ğŸ”§ ä¸“ä¸šï¼šä»˜è´¹æ¨¡å‹ï¼ŒåŠŸèƒ½æœ€å¼º")
        
        if task_type.value in model.strengths:
            reasons.append(f"âœ… æ“…é•¿{task_type.value}ä»»åŠ¡")
        
        if complexity > 0.7:
            reasons.append("ğŸ§  é€‚é…å¤æ‚ä»»åŠ¡")
        elif complexity < 0.3:
            reasons.append("âš¡ é€‚åˆç®€å•ä»»åŠ¡")
        
        if model.speed == "fastest":
            reasons.append("ğŸš€ æé€Ÿå“åº”")
        elif model.speed == "fast":
            reasons.append("ğŸ’¨ å¿«é€Ÿå“åº”")
        
        if model.cost_per_1k_tokens < 0.001:
            reasons.append("ğŸ’µ é›¶æˆæœ¬/ä½æˆæœ¬")
        elif model.cost_per_1k_tokens < 0.01:
            reasons.append("ğŸ’µ ä½æˆæœ¬")
        
        return " | ".join(reasons)
    
    def list_models(self) -> List[Dict]:
        return [
            {
                "id": m.id,
                "name": m.name,
                "provider": m.provider,
                "priority": m.priority.name,
                "speed": m.speed,
                "cost_per_1k_tokens": m.cost_per_1k_tokens,
                "strengths": m.strengths,
                "available": m.available,
            }
            for m in self.MODELS.values()
        ]
    
    def get_profile_name(self, model: Model) -> str:
        profile_map = {
            "gemini-1.5-pro": "research",
            "gemini-2.0-pro": "research",
            "gemini-1.5-flash": "fast",
            "gemini-1.5-flash-8b": "fast",
            "claude-3.5-sonnet": "coding",
            "claude-3.7-sonnet": "coding",
            "deepseek-chat": "crawler",
            "qwen-2.5-72b": "cn",
            "gpt-4o": "coding",
            "gpt-4o-mini": "fast",
        }
        return profile_map.get(model.id, "research")
    
    def activate(self, task: str) -> bool:
        analyzer = TaskAnalyzer(task)
        task_type = analyzer.classify()
        
        model, reason = self.select(task)
        profile = self.get_profile_name(model)
        
        try:
            print(f"\nğŸ¤– æ™ºèƒ½é€‰æ‹©å®Œæˆï¼Œæ­£åœ¨åˆ‡æ¢åˆ° {model.name}...")
            
            dispatcher = SmartModelDispatcher()
            success = dispatcher.activate_profile(profile)
            
            if success:
                print(f"\nâœ… åˆ‡æ¢æˆåŠŸï¼")
                print(f"ğŸ¯ æ¨¡å‹: {model.name}")
                print(f"ğŸ¢ æä¾›å•†: {model.provider}")
                print(f"ğŸ’° æˆæœ¬: ${model.cost_per_1k_tokens:.4f}/1K tokens")
                print(f"\nğŸ’¡ {reason}")
                return True
            else:
                print(f"\nâŒ åˆ‡æ¢å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"\nâŒ åˆ‡æ¢é”™è¯¯: {e}")
            return False


def main():
    json_output = False
    if len(sys.argv) > 1 and sys.argv[1] == "--json":
        json_output = True
        sys.argv.remove("--json")
    
    if len(sys.argv) < 2:
        if json_output:
            print(json.dumps({"error": "è¯·æä¾›ä»»åŠ¡æè¿°"}))
            sys.exit(1)
        print("ç”¨æ³•: python3 model_selector.py [é€‰é¡¹] <ä»»åŠ¡æè¿°>")
        print("")
        print("é€‰é¡¹:")
        print("  --json     JSON æ ¼å¼è¾“å‡ºï¼ˆä¾›è„šæœ¬è°ƒç”¨ï¼‰")
        print("")
        print("ç¤ºä¾‹:")
        print("  python3 model_selector.py 'å¸®æˆ‘å†™ä¸€ä¸ª Python æ’åºç®—æ³•'")
        print("  python3 model_selector.py --json 'å¸®æˆ‘ç¿»è¯‘'")
        print("")
        print("ä¸­æ–‡å…³é”®è¯æ”¯æŒ:")
        print("  ç¼–ç¨‹: å†™ä»£ç ã€å†™ç¨‹åºã€å†™å‡½æ•°ã€å¼€å‘")
        print("  åˆ†æ: åˆ†æã€æ£€æŸ¥ã€å®¡æŸ¥ã€ä¼˜åŒ–")
        print("  è°ƒè¯•: é”™è¯¯ã€ä¿®å¤ã€å´©æºƒã€bug")
        print("  å†™ä½œ: å†™æ–‡æ¡£ã€å†™æ–‡ç« ã€å†™åšå®¢")
        print("  ç¿»è¯‘: ç¿»è¯‘ã€ä»€ä¹ˆæ„æ€ã€æ€ä¹ˆå†™")
        print("  èŠå¤©: ä½ å¥½ã€åœ¨å—ã€èŠèŠ")
        sys.exit(1)
    
    task = " ".join(sys.argv[1:])
    
    selector = SmartModelSelector()
    model, reason = selector.select(task)
    
    if json_output:
        result = {
            "model": f"{model.provider}/{model.id}",
            "provider": model.provider,
            "name": model.name,
            "reason": reason,
            "cost_per_1k_tokens": model.cost_per_1k_tokens,
            "context_window": model.context_window,
            "speed": model.speed,
            "strengths": model.strengths,
        }
        print(json.dumps(result, ensure_ascii=False, indent=2))
        return
    
    print(f"\n{Colors.magenta('ğŸ¤–')} {Colors.bold('æ™ºèƒ½æ¨¡å‹é€‰æ‹©ç»“æœ')}")
    print(Colors.cyan("=" * 60))
    print(f"{Colors.yellow('ğŸ“')} ä»»åŠ¡: {task}")
    print(f"\n{Colors.green('ğŸ¯')} æ¨èæ¨¡å‹: {Colors.bold(model.name)}")
    print(f"{Colors.yellow('ğŸ¢')} æä¾›å•†: {model.provider}")
    print(f"{Colors.yellow('ğŸ’°')} æˆæœ¬: ${model.cost_per_1k_tokens:.4f}/1K tokens")
    print(f"{Colors.yellow('ğŸ“')} ä¸Šä¸‹æ–‡: {model.context_window:,} tokens")
    print(f"{Colors.yellow('ğŸš€')} é€Ÿåº¦: {model.speed}")
    print(f"\n{Colors.cyan('ğŸ’¡')} é€‰æ‹©ç†ç”±: {reason}")
    print(f"\n{Colors.green('âœ…')} æ“…é•¿: {', '.join(model.strengths)}")
    print(Colors.cyan("=" * 60))
    print(f"\n{Colors.cyan('ğŸ’¡')} æç¤º: ç›´æ¥ä½¿ç”¨ op.sh è‡ªåŠ¨åˆ‡æ¢")
    print(f"   ç¤ºä¾‹: op '{task[:30]}...'")


if __name__ == "__main__":
    main()
